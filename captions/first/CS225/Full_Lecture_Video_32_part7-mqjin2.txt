So we run each of these through the new hash function and they may end up in a completely different location in the new array. But when we go looking for them, we know we can find them. Okay any question about that? Otherwise though its exactly parallel. We don't fill the array, we don't exactly double the table size, we don't copy the data, but we have analogous parallel things to do in each of those cases. That process is called rehashing. Twice the array size. So am I worried about having to find the first prime greater than twice the table size? No, we can find primes in a table really quickly. you can store a lot of primes for lookup, you dont have to compute them.  Certainly enough to create an array of that size. So the question is where does the 2/3 come from and does it only strictly apply to linear probing and double hashing? It's the load factor that corresponds to probe based hashing strategies generally, however they are implemented. So probably its a double hashing, in general you'll let your table be 2/3 full. And it gives you sort of a feeling of what's going on behind the scenes when you use a hash map as well. So the question is, is there any way of escaping the fact that we are intentionally allocating memory that we aren't going to use. We admit going in  that 1/3 of that space is not going to be used. I believe the answer to that is no. You have to allow for data to be mapped into those cells, otherwise you got a different problem. That's equivalent to having a full array, and then you got issues. So, where are we? We got these two collision resolution strategies, one of them just sort of hangs out the data, the other one uses probing to place it in the table. There are a gazillion others that you might use. For example, for separate chaining, instead of a chain, why not use another hash table, or why not use another AVL tree? Why not use something else instead of just a singly linked list. So the number of variability in that particular structure is astounding. So these are just sort of the basic ideas. Now, given those basic ideas, which one is better? The structure speed is way faster for probe based hashing. That is, if you can actually fit all your data in an array, that structure itself will be much faster. But if your data is big, then you can't create an array that's very big using your memory effectively, and so you'll want to have the ability to hang your data off the structure. So if you have big records, then separate chaining is a better strategy. And of course this is a hugs simplification. What structures do hash tables replace for us? What do we use them to implement? Dictionaries. What did we use to implement dictionaries before?