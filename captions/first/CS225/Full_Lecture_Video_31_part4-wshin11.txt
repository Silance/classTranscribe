Um, if you're data is in a linked, if your data all maps to the same array cell, um, you have several different ways of handling it, which we'll talk about, all of which imply you have to search for your data within that cell. Okay? And so, you might as well just to put 'em in a big bucket or a list, or whatever. Okay? Yes, right, that's right. Okay, so I want to rehash things again. Is the moment for the hash brown joke? I don't know, it's not right now. Okay, alright. Um, so I wanna revisit a couple things just to convince you that hashing is hard. So this time I drew the same picture but with the key space small in the first example. So, I wanna reinforce the idea that if you happen to know exactly what you're data is going to be, then by all means figure out a way of mapping your data into an array via a bijection. Okay? That's on you. That's on you. You can do that, nobody's telling you, oh don't try to do that on your own. If your data is small enough, it's usually kind of a clever, you know, clever little puzzle to figure out ways of putting your data efficiently into a table. It's, um, it's figuring out identifiers for your keys that allow you to represent those keys using indices from an array. Alright, now why? nuuuuuung Okay. Uh, using indices as the representatives here. So, the punchline is if your key space is small, by all means, make your own bijection. Have fun, it's fun. Okay, on the other hand, if your key space is large or very general, that is you don't really even necessarily know what the key space looks like, which is the case for a lot of general purpose hash functions provided for you by a system, like C++'s hash map, then the puzzle becomes harder. Because, every set of data that you choose from that hash function, it might not be a very big set of data, certainly it should fit in your table, right? Certainly should have memory to contain all of your data, but you cannot control what subset you get. So it might be the case that one subset will spread the data out nicely over the table, but, and maybe that will be the case for couple different subsets that happen to constitute your data for an application, but what you cannot guarantee based on arguments related to the pigeonhole principle is that no hash function can do well on all subsets. There are just too many of them, too many possible subsets, especially of a large key space. You cannot guarantee good behavior from a, over all subsets... from a general purpose