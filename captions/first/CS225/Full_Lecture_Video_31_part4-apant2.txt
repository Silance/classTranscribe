Um, if you're data is in a ... if your data all maps to the same, um, array cell, um, you have several different ways of handling it which we'll talk about. All of which imply you have to search for your data within that cell. Okay, and so you might as well just to put them in a big bucket or a list or whatever, okay? [inaudible] Yes, right that's right. Okay, so I want to rehash things again [inaudible]. Is this the moment for the hash brown joke? I don't know, it's not right now, okay. All right, um so I want to revisit a couple of things just to convince you that hashing is hard. So, this time I drew the same picture but with the keyspace small in the first example, kay? So i want to reinforce the idea that if you happen to know exactly what you're data is going to be, then by all means figure out a way of mapping your data into an array via a bijection, okay? That's on you, that's on you. You can do that, nobody is telling you, oh don't try to do that on your own. Your data is small enough, its usually kind of a clever, you know, clever little puzzle to figure out ways of putting your data efficiently into a table. It's, um, it's figuring out identifiers for your keys that allow you to represent those keys using indices from an array. Alright, now why...[Inaudible]. Kay, uh using indices as the, um, representatives here, kay? So the punchline is if your key space is small, by all means make your own bijection; have fun, it's fun. Okay, on the other hand if your key space is large or very general, that is you don't really even necessarily know what the key space looks like, which is the case for, um, a lot of general purpose hash function provided for you by a system like C++'s hashmap, then the puzzle becomes harder. Because, every set of data that you choose from that hash function it might not be a very big set of data, certainly it should fir in your table, right? Certainly you should have enough memory to contain all of your data, but you cannot control what subset you get so it might be the case that one subset will spread the data out nicely over the table, but some and, and maybe that would be the case for, you know, [a] couple different subsets that happen to constitute your data for an application. But what you cannot guarantee based on arguments related to the pigeonhole principle is that no hash function can do well on all subsets. There are just too many of them, too many possible subsets especially of a large keyspace. So, you cannot guarantee good behavior from a, uh, overall subsets from a general purpose.