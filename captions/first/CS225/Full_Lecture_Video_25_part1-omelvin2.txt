Ok, so Mike described - what Mike described was that oh please let me write on my screen. no it won't let me. Ok. what mike described was that 36 would be swapped up in place of 35 and the order of the values would not change. so so we had this small local set of data. so if i wrote down all the data in the tree in order, it would have looked like this. 23 35 36 54 74 etc. so if I had written down all the keys in the tree they would've looked something like this. now our goal was to remove 35. what we're saying is ok fine, go ahead and remove 35. we can do that if we either sort of let 23 or 36 take its place. ok. now we have special names for these things. the name we have for 23 is the in order predecessor. and the name we have for 36 is the in order successor. and these are in the slides from last time. now, you can't read this I'm sure because I can't. if you look at this applet and you go look at the menu you can choose whether or not you want to do a removal using the in order predecessor or in order successor. so this is recognized as a design decision that you're going to make. now I made the design decision to use in order predecessor because the textbook I used to use used in order predecessor. textbooks on data structures are about evenly split between IOP and IOS. but both of them are kind of wrong. because the consequence of always using one or the other if you're doing a lot of removals is that you get a tree that's kind of lopsided. if removals always happen from the left then the trees going to be heavier in general on the right. so you end up inducing an imbalance in the structure just by virtue of the mechanism you use for removal. real algorithms to do this would probably swap back and forth between in order predecessor and in order successor. yes, quickly. maybe. maybe. some definition of balance would come into play. it might be the height of the other subtrees and there are other metrics on trees you can use to assess the size of a subtree. k. balance almost always means that the sizes of subtrees are approximately the same. the question is how is size defined. is it height, is it number of nodes, is it some other metric on the size of the tree? that's for discussion and um. ok how is it? is it one finger if I do this? no. huh, yes. ok look. this is great. k. but I don't know why, ok fine. whoo, whoo, ok, fine, whatever. ok, any questions about all that? so I just wanted to remind that you that applet exists. i think i find it particularly useful. we'll use it again. ok, um, running times - so we're talking about implementing dictionaries using binary search trees. so far any time we talk about implementations we're responsible for arguing about the efficiency of that design choice. that implementation choice. right now, this moment, the only thing we have right now to argue the running times of these algorithms are - we - we know they all use the height of the tree. they all traverse the height of the tree. they all walk down in the worst case. in the worst case the height of the tree is going to determine the running time. any question about that? alright, and then traversal of course just by way of reminder is big o(n) the amount of data in the structure. so, let's see. i want to know, this is just a little bit of practice. i want to know what this tree looks like. so really quickly declare my t to be a binary  search tree. and go ahead and on paper, as quickly as you can implement that sequence of inserts. ok. will you do yours on my tablet? it doesn't matter you can make as many mistakes as you want. ok. so everybody do this, just build the tree really quickly. insert into the binary tree several times.