[{"text":"So we run each of these through the new hash function, and they may end up in a completely different location in the new array.","width":962},{"text":"But when we go looking for them we know we can find them.","width":271},{"text":"Okay, any question about that?","width":138},{"text":"Alright so otherwise though it's exactly parallel.","width":183},{"text":"We don't fill the array.","width":97},{"text":"We don't exactly double the table size and we don't copy the data but we have analogous, parallel things to do in each of those cases.","width":636},{"text":"That process is called re-hashing.","width":260},{"text":"Questions?","width":73},{"text":"Is it okay?","width":342},{"text":"Twice the array size.","width":571},{"text":"So am I worried about having to find the first prime?","width":212},{"text":"Greater than twice the table size?","width":245},{"text":"No.","width":44},{"text":"We can find primes in a table really quickly.","width":248},{"text":"You can store a lot of primes for lookup, you don't have to compute them.","width":332},{"text":"Certainly enough to create an array of that size, yeah?","width":653},{"text":"So in practice it's about two thirds.","width":131},{"text":"So the question is where did the two thirds come from and does it only strictly apply to linear probing and double hashing?","width":445},{"text":"It's the load factor that corresponds to probe based hashing strategies generally however they are implemented.","width":720},{"text":"So probably it's a double hashing.","width":166},{"text":"In general you let your table be two thirds full.","width":453},{"text":"It gives you sort of a feeling for what's going on behind the scenes when you use a hash map as well.","width":598},{"text":"So the question is, is there any way of escaping the fact that we are intentionally allocating memory that we aren't going to use we admit, we admit going in that a third of that space is not going to be used, and I believe the answer to that is \"no".","width":1596},{"text":"Right?","width":19},{"text":"That you have to allow for data to be mapped into those cells otherwise you've got a different problem.","width":414},{"text":"That's sorta equivalent to having a full array and then you've got issues.","width":380},{"text":"Alright any other questions about this?","width":444},{"text":"So, where are we?","width":119},{"text":"We got these two collision resolution strategies.","width":292},{"text":"One of them just sort of hangs out the data.","width":160},{"text":"The other one uses probing to place it in the table.","width":264},{"text":"There are a gazillion others that you might use, that might be use.d For example.","width":545},{"text":"For separate chaining instead of a chain, why not use another hash table, or why not use an AVL tree?","width":450},{"text":"Why not use something else instead of just a singly linked list?","width":304},{"text":"So the number of variety, the amount of variability in that particular structure is astounding.","width":653},{"text":"Okay, so these are just sort of the basic ideas.","width":289},{"text":"Now, given those basic ideas, which one is better?","width":354},{"text":"The structure speed is way faster for probe based hashing.","width":338},{"text":"That is, if you can actually fit all your data in an array, that structure itself will be much faster.","width":1027},{"text":"But if your data is big, then you can't create an array that's very big using your memory effectively, and so you'll want to have the ability to hang your data off the structure.","width":829},{"text":"So if you have big records then separate chaining is a better strategy.","width":481},{"text":"And of course this is a huge simplification.","width":191},{"text":"Okay, what structures do hash tables replace for us?","width":468},{"text":"What do we use them to implement?","width":168},{"text":"Dictionaries.","width":51},{"text":"What did we use to implement dictionaries before?","width":342}]